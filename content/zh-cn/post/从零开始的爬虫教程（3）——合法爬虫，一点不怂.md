---
title: 从零开始的爬虫教程（3）——合法爬虫，一点不怂
date: 2020-01-26
draft: false
description: 在前一篇文章中我介绍了多线程爬取网站的方法。实际上，单线程一次一次地获取网站页面和所谓“爬取”大相径庭，更不如说是“没有浏览器的访问”，而当我们采用了并发多线程的时候就成为了“爬取”。爬取信息需要遵守互联网法律法规，这篇文章我将会介绍一些爬虫要遵循的一些基本规定。
image: https://futrime.gitee.io/image-cdn/2020/02/2531887038.jpg
author: Futrime
categories:
  - 奇技淫巧
  - 经验杂谈
tags:
  - 爬虫
  - 合法
  - Robots.txt
---

去年6月份中国颁布了[《网络爬虫的法律规制》][3]，进一步明确了爬虫侵权的主体责任。因此在当下，合法有序而又高效地爬取显得尤为重要。现在大部分网站都遵守拒绝蜘蛛协议（The Robots Exclusion Protocol），在网站根目录下往往会有`robots.txt`文件，例如[TechPowerUp][4]。该文件详细阐述了网站中哪些内容可以被爬取，哪些内容不可以被爬取。这篇文章我将会以[TechPowerUp][5]为例，简要讲讲/robots.txt协议规则。

如下是TechPowerUp的`/robots.txt`文件
    User-agent: *
    Disallow: /xf2/
    Disallow: /wizzard/
其中的User-agent代表此规则适用的UA类型，例如必应的`User-Agent: Bingbot`。我们可以在浏览器开发者工具中查看自己的Request包头中的User-Agent字段，如下图所示。

![1.png][6]

我的是`Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0`。

一般来说，我们只需要关注`User-agent: *`字段下的规则。Disallow规则指明了不可以访问的目录，如上例中的`/xf2/`和`/wizzard/`就是我们不可以访问的，在我们爬取的时候要记住不可以爬取这些内容。有一些网站会用Allow字段，那么就是除了这个字段里表明的目录以外都不可以爬取。（但是实际上只要不把爬取的信息公开、泄漏或商用、违法用途，一般都没有问题）

遵守好以上几条规则，我们便可以高枕无忧了。注意我们爬下的数据千万不要违法公开泄露、非法商用！除非你想要让自己暴露在律师函警告和传票的威胁之下，请参考：[只因写了一段爬虫，公司200多人被抓！][7]。

这篇文章基本上就是水过去了，在下一篇文章中我将会介绍一些关于“合法”规避网站反爬虫拦截的方法。

参考链接：
1. https://www.techpowerup.com
1. https://www.robotstxt.org/robotstxt.html
1. https://en.wikipedia.org/wiki/Robots_exclusion_standard


  [1]: https://blog.futrime.com/skills/93.html
  [3]: http://www.cac.gov.cn/2019-06/16/c_1124630015.htm
  [4]: https://www.techpowerup.com/robots.txt
  [5]: https://www.techpowerup.com/robots.txt
  [6]: https://futrime.gitee.io/image-cdn/2020/01/1772645195.png
  [7]: https://zhuanlan.zhihu.com/p/86948606
